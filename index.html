<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research Paper</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.16/dist/tailwind.min.css" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/daisyui@1.3.7/full.css" rel="stylesheet">
</head>
<body class="bg-gray-100" style="background-color: #0063a6;">
  <div class="container mx-auto px-4 py-8">
    <div class="bg-white rounded-lg shadow p-6">
      <h1 class="text-2xl font-bold mb-4">Adaptive Precision Training (AdaPT): A dynamic quantized training approach for DNNs</h1>
      <h2 class="text-xl font-semibold mb-2">Abstract</h2>
      <p class="mb-4">
        Quantizing deep neural networks (DNNs) is an important strategy for training or inference in time critical applications. State-of-the-art quantization approaches focus on post-training quantization. While some work on quantization during training exists, most approaches require refinement in full precision (usually single precision) in the final training phase, use a rather coarse quantization, that leads to a loss in accuracy, or enforce a global bit-width across the entire DNN. This leads to suboptimal assignments of bit-widths to layers and, consequently, suboptimal resource usage. To overcome such limitations, we introduce AdaPT, a new fixed- point quantized sparsifying training strategy for deep neural networks. AdaPT decides about precision switches between training epochs based on an information theory motivated heuristic. On a per-layer basis, AdaPT chooses the lowest precision that causes no quantization-induced information loss, while keeping the precision high enough such that future learning steps do not suffer from vanishing gradients. The benefits of this quantization are evaluated based on an analytical performance model. We illustrate an average 1.31 × (or 4.76× adjusted for iso-accuracy) speedup compared to standard training in float32 at iso-accuracy, even achieving an average accuracy increase of 0.74 percentage points for AlexNet/ResNet-20 on CIFAR10/CIFAR100/EMNIST and LeNet-5/MNIST. We demonstrate that these trained models reach an average inference 2.28× speedup with a model size reduction up to 51% of the corresponding unquantized model.
      </p>
      <h2 class="text-xl font-semibold mb-2">Authors</h2>
      <p class="mb-4">
        Lorenz Kummer, <a href="https://www.sidak.xyz">Kevin Sidak</a>, Tabea Reichmann, Wilfried Gansterer
      </p>
      <h2 class="text-xl font-semibold mb-2">Key Findings</h2>
      <ul class="list-disc list-inside mb-4">
        <li>AdaPT employs a fixed-point representation for precision switching, which can be integrated with iterative gradient-based training.</li>
        <li>It balances efficient fixed-point operations for forward passes and gradient computation with high-precision float32 weight updates, resulting in optimal training speed while maintaining accuracy.</li>
        <li>AdaPT outperforms <a href="https://arxiv.org/abs/2006.09049">MuPPET</a> in mixed-precision training, offering faster training with minimal loss in accuracy.</li>
        <li>The inference acceleration provided by AdaPT is not as competitive as state-of-the-art low-precision approaches, but it offers advantages over <a href="https://arxiv.org/abs/2006.09049">MuPPET</a> in terms of speedup and memory consumption.</li>
        <li>The trade-offs between training and inference acceleration, memory consumption, and model accuracy are of interest for future work.</li>
      </ul>
      <a href="https://epubs.siam.org/doi/10.1137/1.9781611977653.ch63" class="text-blue-500 
hover:text-blue-700" target="_blank" rel="noopener noreferrer">Read the full paper</a>
    </div>
  </div>
</body>
</html>

